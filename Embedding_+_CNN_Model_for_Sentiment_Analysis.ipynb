{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Embedding + CNN Model for Sentiment Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-n6W1ThXzND"
      },
      "source": [
        "Word embeddings are a technique for representing text where different words with similar meaning have a similar real-valued vector representation. \n",
        "\n",
        "They are a key breakthrough that has led to great performance of neural network models on a suite of challenging natural language processing problems. In this project, we will discover how to develop word embedding models with convolutional neural networks to classify movie reviews. \n",
        "\n",
        "**Steps:** \n",
        "\n",
        "1.  prepare movie review text data \n",
        "   forclassification with deep learning methods.\n",
        "\n",
        "2. develop a neural classification\n",
        "   model with word embedding and \n",
        "   convolutional layers. \n",
        " \n",
        "3. evaluate the developed a \n",
        "   neural classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiZeMrNelABp"
      },
      "source": [
        "# importing libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkTTnSH2KMt9"
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idv228vklwxE"
      },
      "source": [
        "# downloading data from website an unzipping using keras/tensorflow \n",
        "zip_path=tf.keras.utils.get_file(origin='https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz',\n",
        "                                 fname='review_polarity.tar.gz',\n",
        "                                 extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cgUOCd7JFDW",
        "outputId": "b70d99f3-65ac-47db-eab2-e6d266c8b5cf"
      },
      "source": [
        "!ls /root/.keras/datasets/txt_sentoken"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg  pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWJOh5VQJQi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b63cc5-dcf4-4d84-8083-85b7182544f3"
      },
      "source": [
        "!ls /root/.keras/datasets/txt_sentoken/neg/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cv000_29416.txt  cv250_26462.txt  cv500_10722.txt  cv750_10606.txt\n",
            "cv001_19502.txt  cv251_23901.txt  cv501_12675.txt  cv751_17208.txt\n",
            "cv002_17424.txt  cv252_24974.txt  cv502_10970.txt  cv752_25330.txt\n",
            "cv003_12683.txt  cv253_10190.txt  cv503_11196.txt  cv753_11812.txt\n",
            "cv004_12641.txt  cv254_5870.txt   cv504_29120.txt  cv754_7709.txt\n",
            "cv005_29357.txt  cv255_15267.txt  cv505_12926.txt  cv755_24881.txt\n",
            "cv006_17022.txt  cv256_16529.txt  cv506_17521.txt  cv756_23676.txt\n",
            "cv007_4992.txt\t cv257_11856.txt  cv507_9509.txt   cv757_10668.txt\n",
            "cv008_29326.txt  cv258_5627.txt   cv508_17742.txt  cv758_9740.txt\n",
            "cv009_29417.txt  cv259_11827.txt  cv509_17354.txt  cv759_15091.txt\n",
            "cv010_29063.txt  cv260_15652.txt  cv510_24758.txt  cv760_8977.txt\n",
            "cv011_13044.txt  cv261_11855.txt  cv511_10360.txt  cv761_13769.txt\n",
            "cv012_29411.txt  cv262_13812.txt  cv512_17618.txt  cv762_15604.txt\n",
            "cv013_10494.txt  cv263_20693.txt  cv513_7236.txt   cv763_16486.txt\n",
            "cv014_15600.txt  cv264_14108.txt  cv514_12173.txt  cv764_12701.txt\n",
            "cv015_29356.txt  cv265_11625.txt  cv515_18484.txt  cv765_20429.txt\n",
            "cv016_4348.txt\t cv266_26644.txt  cv516_12117.txt  cv766_7983.txt\n",
            "cv017_23487.txt  cv267_16618.txt  cv517_20616.txt  cv767_15673.txt\n",
            "cv018_21672.txt  cv268_20288.txt  cv518_14798.txt  cv768_12709.txt\n",
            "cv019_16117.txt  cv269_23018.txt  cv519_16239.txt  cv769_8565.txt\n",
            "cv020_9234.txt\t cv270_5873.txt   cv520_13297.txt  cv770_11061.txt\n",
            "cv021_17313.txt  cv271_15364.txt  cv521_1730.txt   cv771_28466.txt\n",
            "cv022_14227.txt  cv272_20313.txt  cv522_5418.txt   cv772_12971.txt\n",
            "cv023_13847.txt  cv273_28961.txt  cv523_18285.txt  cv773_20264.txt\n",
            "cv024_7033.txt\t cv274_26379.txt  cv524_24885.txt  cv774_15488.txt\n",
            "cv025_29825.txt  cv275_28725.txt  cv525_17930.txt  cv775_17966.txt\n",
            "cv026_29229.txt  cv276_17126.txt  cv526_12868.txt  cv776_21934.txt\n",
            "cv027_26270.txt  cv277_20467.txt  cv527_10338.txt  cv777_10247.txt\n",
            "cv028_26964.txt  cv278_14533.txt  cv528_11669.txt  cv778_18629.txt\n",
            "cv029_19943.txt  cv279_19452.txt  cv529_10972.txt  cv779_18989.txt\n",
            "cv030_22893.txt  cv280_8651.txt   cv530_17949.txt  cv780_8467.txt\n",
            "cv031_19540.txt  cv281_24711.txt  cv531_26838.txt  cv781_5358.txt\n",
            "cv032_23718.txt  cv282_6833.txt   cv532_6495.txt   cv782_21078.txt\n",
            "cv033_25680.txt  cv283_11963.txt  cv533_9843.txt   cv783_14724.txt\n",
            "cv034_29446.txt  cv284_20530.txt  cv534_15683.txt  cv784_16077.txt\n",
            "cv035_3343.txt\t cv285_18186.txt  cv535_21183.txt  cv785_23748.txt\n",
            "cv036_18385.txt  cv286_26156.txt  cv536_27221.txt  cv786_23608.txt\n",
            "cv037_19798.txt  cv287_17410.txt  cv537_13516.txt  cv787_15277.txt\n",
            "cv038_9781.txt\t cv288_20212.txt  cv538_28485.txt  cv788_26409.txt\n",
            "cv039_5963.txt\t cv289_6239.txt   cv539_21865.txt  cv789_12991.txt\n",
            "cv040_8829.txt\t cv290_11981.txt  cv540_3092.txt   cv790_16202.txt\n",
            "cv041_22364.txt  cv291_26844.txt  cv541_28683.txt  cv791_17995.txt\n",
            "cv042_11927.txt  cv292_7804.txt   cv542_20359.txt  cv792_3257.txt\n",
            "cv043_16808.txt  cv293_29731.txt  cv543_5107.txt   cv793_15235.txt\n",
            "cv044_18429.txt  cv294_12695.txt  cv544_5301.txt   cv794_17353.txt\n",
            "cv045_25077.txt  cv295_17060.txt  cv545_12848.txt  cv795_10291.txt\n",
            "cv046_10613.txt  cv296_13146.txt  cv546_12723.txt  cv796_17243.txt\n",
            "cv047_18725.txt  cv297_10104.txt  cv547_18043.txt  cv797_7245.txt\n",
            "cv048_18380.txt  cv298_24487.txt  cv548_18944.txt  cv798_24779.txt\n",
            "cv049_21917.txt  cv299_17950.txt  cv549_22771.txt  cv799_19812.txt\n",
            "cv050_12128.txt  cv300_23302.txt  cv550_23226.txt  cv800_13494.txt\n",
            "cv051_10751.txt  cv301_13010.txt  cv551_11214.txt  cv801_26335.txt\n",
            "cv052_29318.txt  cv302_26481.txt  cv552_0150.txt   cv802_28381.txt\n",
            "cv053_23117.txt  cv303_27366.txt  cv553_26965.txt  cv803_8584.txt\n",
            "cv054_4101.txt\t cv304_28489.txt  cv554_14678.txt  cv804_11763.txt\n",
            "cv055_8926.txt\t cv305_9937.txt   cv555_25047.txt  cv805_21128.txt\n",
            "cv056_14663.txt  cv306_10859.txt  cv556_16563.txt  cv806_9405.txt\n",
            "cv057_7962.txt\t cv307_26382.txt  cv557_12237.txt  cv807_23024.txt\n",
            "cv058_8469.txt\t cv308_5079.txt   cv558_29376.txt  cv808_13773.txt\n",
            "cv059_28723.txt  cv309_23737.txt  cv559_0057.txt   cv809_5012.txt\n",
            "cv060_11754.txt  cv310_14568.txt  cv560_18608.txt  cv810_13660.txt\n",
            "cv061_9321.txt\t cv311_17708.txt  cv561_9484.txt   cv811_22646.txt\n",
            "cv062_24556.txt  cv312_29308.txt  cv562_10847.txt  cv812_19051.txt\n",
            "cv063_28852.txt  cv313_19337.txt  cv563_18610.txt  cv813_6649.txt\n",
            "cv064_25842.txt  cv314_16095.txt  cv564_12011.txt  cv814_20316.txt\n",
            "cv065_16909.txt  cv315_12638.txt  cv565_29403.txt  cv815_23466.txt\n",
            "cv066_11668.txt  cv316_5972.txt   cv566_8967.txt   cv816_15257.txt\n",
            "cv067_21192.txt  cv317_25111.txt  cv567_29420.txt  cv817_3675.txt\n",
            "cv068_14810.txt  cv318_11146.txt  cv568_17065.txt  cv818_10698.txt\n",
            "cv069_11613.txt  cv319_16459.txt  cv569_26750.txt  cv819_9567.txt\n",
            "cv070_13249.txt  cv320_9693.txt   cv570_28960.txt  cv820_24157.txt\n",
            "cv071_12969.txt  cv321_14191.txt  cv571_29292.txt  cv821_29283.txt\n",
            "cv072_5928.txt\t cv322_21820.txt  cv572_20053.txt  cv822_21545.txt\n",
            "cv073_23039.txt  cv323_29633.txt  cv573_29384.txt  cv823_17055.txt\n",
            "cv074_7188.txt\t cv324_7502.txt   cv574_23191.txt  cv824_9335.txt\n",
            "cv075_6250.txt\t cv325_18330.txt  cv575_22598.txt  cv825_5168.txt\n",
            "cv076_26009.txt  cv326_14777.txt  cv576_15688.txt  cv826_12761.txt\n",
            "cv077_23172.txt  cv327_21743.txt  cv577_28220.txt  cv827_19479.txt\n",
            "cv078_16506.txt  cv328_10908.txt  cv578_16825.txt  cv828_21392.txt\n",
            "cv079_12766.txt  cv329_29293.txt  cv579_12542.txt  cv829_21725.txt\n",
            "cv080_14899.txt  cv330_29675.txt  cv580_15681.txt  cv830_5778.txt\n",
            "cv081_18241.txt  cv331_8656.txt   cv581_20790.txt  cv831_16325.txt\n",
            "cv082_11979.txt  cv332_17997.txt  cv582_6678.txt   cv832_24713.txt\n",
            "cv083_25491.txt  cv333_9443.txt   cv583_29465.txt  cv833_11961.txt\n",
            "cv084_15183.txt  cv334_0074.txt   cv584_29549.txt  cv834_23192.txt\n",
            "cv085_15286.txt  cv335_16299.txt  cv585_23576.txt  cv835_20531.txt\n",
            "cv086_19488.txt  cv336_10363.txt  cv586_8048.txt   cv836_14311.txt\n",
            "cv087_2145.txt\t cv337_29061.txt  cv587_20532.txt  cv837_27232.txt\n",
            "cv088_25274.txt  cv338_9183.txt   cv588_14467.txt  cv838_25886.txt\n",
            "cv089_12222.txt  cv339_22452.txt  cv589_12853.txt  cv839_22807.txt\n",
            "cv090_0049.txt\t cv340_14776.txt  cv590_20712.txt  cv840_18033.txt\n",
            "cv091_7899.txt\t cv341_25667.txt  cv591_24887.txt  cv841_3367.txt\n",
            "cv092_27987.txt  cv342_20917.txt  cv592_23391.txt  cv842_5702.txt\n",
            "cv093_15606.txt  cv343_10906.txt  cv593_11931.txt  cv843_17054.txt\n",
            "cv094_27868.txt  cv344_5376.txt   cv594_11945.txt  cv844_13890.txt\n",
            "cv095_28730.txt  cv345_9966.txt   cv595_26420.txt  cv845_15886.txt\n",
            "cv096_12262.txt  cv346_19198.txt  cv596_4367.txt   cv846_29359.txt\n",
            "cv097_26081.txt  cv347_14722.txt  cv597_26744.txt  cv847_20855.txt\n",
            "cv098_17021.txt  cv348_19207.txt  cv598_18184.txt  cv848_10061.txt\n",
            "cv099_11189.txt  cv349_15032.txt  cv599_22197.txt  cv849_17215.txt\n",
            "cv100_12406.txt  cv350_22139.txt  cv600_25043.txt  cv850_18185.txt\n",
            "cv101_10537.txt  cv351_17029.txt  cv601_24759.txt  cv851_21895.txt\n",
            "cv102_8306.txt\t cv352_5414.txt   cv602_8830.txt   cv852_27512.txt\n",
            "cv103_11943.txt  cv353_19197.txt  cv603_18885.txt  cv853_29119.txt\n",
            "cv104_19176.txt  cv354_8573.txt   cv604_23339.txt  cv854_18955.txt\n",
            "cv105_19135.txt  cv355_18174.txt  cv605_12730.txt  cv855_22134.txt\n",
            "cv106_18379.txt  cv356_26170.txt  cv606_17672.txt  cv856_28882.txt\n",
            "cv107_25639.txt  cv357_14710.txt  cv607_8235.txt   cv857_17527.txt\n",
            "cv108_17064.txt  cv358_11557.txt  cv608_24647.txt  cv858_20266.txt\n",
            "cv109_22599.txt  cv359_6751.txt   cv609_25038.txt  cv859_15689.txt\n",
            "cv110_27832.txt  cv360_8927.txt   cv610_24153.txt  cv860_15520.txt\n",
            "cv111_12253.txt  cv361_28738.txt  cv611_2253.txt   cv861_12809.txt\n",
            "cv112_12178.txt  cv362_16985.txt  cv612_5396.txt   cv862_15924.txt\n",
            "cv113_24354.txt  cv363_29273.txt  cv613_23104.txt  cv863_7912.txt\n",
            "cv114_19501.txt  cv364_14254.txt  cv614_11320.txt  cv864_3087.txt\n",
            "cv115_26443.txt  cv365_12442.txt  cv615_15734.txt  cv865_28796.txt\n",
            "cv116_28734.txt  cv366_10709.txt  cv616_29187.txt  cv866_29447.txt\n",
            "cv117_25625.txt  cv367_24065.txt  cv617_9561.txt   cv867_18362.txt\n",
            "cv118_28837.txt  cv368_11090.txt  cv618_9469.txt   cv868_12799.txt\n",
            "cv119_9909.txt\t cv369_14245.txt  cv619_13677.txt  cv869_24782.txt\n",
            "cv120_3793.txt\t cv370_5338.txt   cv620_2556.txt   cv870_18090.txt\n",
            "cv121_18621.txt  cv371_8197.txt   cv621_15984.txt  cv871_25971.txt\n",
            "cv122_7891.txt\t cv372_6654.txt   cv622_8583.txt   cv872_13710.txt\n",
            "cv123_12165.txt  cv373_21872.txt  cv623_16988.txt  cv873_19937.txt\n",
            "cv124_3903.txt\t cv374_26455.txt  cv624_11601.txt  cv874_12182.txt\n",
            "cv125_9636.txt\t cv375_9932.txt   cv625_13518.txt  cv875_5622.txt\n",
            "cv126_28821.txt  cv376_20883.txt  cv626_7907.txt   cv876_9633.txt\n",
            "cv127_16451.txt  cv377_8440.txt   cv627_12603.txt  cv877_29132.txt\n",
            "cv128_29444.txt  cv378_21982.txt  cv628_20758.txt  cv878_17204.txt\n",
            "cv129_18373.txt  cv379_23167.txt  cv629_16604.txt  cv879_16585.txt\n",
            "cv130_18521.txt  cv380_8164.txt   cv630_10152.txt  cv880_29629.txt\n",
            "cv131_11568.txt  cv381_21673.txt  cv631_4782.txt   cv881_14767.txt\n",
            "cv132_5423.txt\t cv382_8393.txt   cv632_9704.txt   cv882_10042.txt\n",
            "cv133_18065.txt  cv383_14662.txt  cv633_29730.txt  cv883_27621.txt\n",
            "cv134_23300.txt  cv384_18536.txt  cv634_11989.txt  cv884_15230.txt\n",
            "cv135_12506.txt  cv385_29621.txt  cv635_0984.txt   cv885_13390.txt\n",
            "cv136_12384.txt  cv386_10229.txt  cv636_16954.txt  cv886_19210.txt\n",
            "cv137_17020.txt  cv387_12391.txt  cv637_13682.txt  cv887_5306.txt\n",
            "cv138_13903.txt  cv388_12810.txt  cv638_29394.txt  cv888_25678.txt\n",
            "cv139_14236.txt  cv389_9611.txt   cv639_10797.txt  cv889_22670.txt\n",
            "cv140_7963.txt\t cv390_12187.txt  cv640_5380.txt   cv890_3515.txt\n",
            "cv141_17179.txt  cv391_11615.txt  cv641_13412.txt  cv891_6035.txt\n",
            "cv142_23657.txt  cv392_12238.txt  cv642_29788.txt  cv892_18788.txt\n",
            "cv143_21158.txt  cv393_29234.txt  cv643_29282.txt  cv893_26731.txt\n",
            "cv144_5010.txt\t cv394_5311.txt   cv644_18551.txt  cv894_22140.txt\n",
            "cv145_12239.txt  cv395_11761.txt  cv645_17078.txt  cv895_22200.txt\n",
            "cv146_19587.txt  cv396_19127.txt  cv646_16817.txt  cv896_17819.txt\n",
            "cv147_22625.txt  cv397_28890.txt  cv647_15275.txt  cv897_11703.txt\n",
            "cv148_18084.txt  cv398_17047.txt  cv648_17277.txt  cv898_1576.txt\n",
            "cv149_17084.txt  cv399_28593.txt  cv649_13947.txt  cv899_17812.txt\n",
            "cv150_14279.txt  cv400_20631.txt  cv650_15974.txt  cv900_10800.txt\n",
            "cv151_17231.txt  cv401_13758.txt  cv651_11120.txt  cv901_11934.txt\n",
            "cv152_9052.txt\t cv402_16097.txt  cv652_15653.txt  cv902_13217.txt\n",
            "cv153_11607.txt  cv403_6721.txt   cv653_2107.txt   cv903_18981.txt\n",
            "cv154_9562.txt\t cv404_21805.txt  cv654_19345.txt  cv904_25663.txt\n",
            "cv155_7845.txt\t cv405_21868.txt  cv655_12055.txt  cv905_28965.txt\n",
            "cv156_11119.txt  cv406_22199.txt  cv656_25395.txt  cv906_12332.txt\n",
            "cv157_29302.txt  cv407_23928.txt  cv657_25835.txt  cv907_3193.txt\n",
            "cv158_10914.txt  cv408_5367.txt   cv658_11186.txt  cv908_17779.txt\n",
            "cv159_29374.txt  cv409_29625.txt  cv659_21483.txt  cv909_9973.txt\n",
            "cv160_10848.txt  cv410_25624.txt  cv660_23140.txt  cv910_21930.txt\n",
            "cv161_12224.txt  cv411_16799.txt  cv661_25780.txt  cv911_21695.txt\n",
            "cv162_10977.txt  cv412_25254.txt  cv662_14791.txt  cv912_5562.txt\n",
            "cv163_10110.txt  cv413_7893.txt   cv663_14484.txt  cv913_29127.txt\n",
            "cv164_23451.txt  cv414_11161.txt  cv664_4264.txt   cv914_2856.txt\n",
            "cv165_2389.txt\t cv415_23674.txt  cv665_29386.txt  cv915_9342.txt\n",
            "cv166_11959.txt  cv416_12048.txt  cv666_20301.txt  cv916_17034.txt\n",
            "cv167_18094.txt  cv417_14653.txt  cv667_19672.txt  cv917_29484.txt\n",
            "cv168_7435.txt\t cv418_16562.txt  cv668_18848.txt  cv918_27080.txt\n",
            "cv169_24973.txt  cv419_14799.txt  cv669_24318.txt  cv919_18155.txt\n",
            "cv170_29808.txt  cv420_28631.txt  cv670_2666.txt   cv920_29423.txt\n",
            "cv171_15164.txt  cv421_9752.txt   cv671_5164.txt   cv921_13988.txt\n",
            "cv172_12037.txt  cv422_9632.txt   cv672_27988.txt  cv922_10185.txt\n",
            "cv173_4295.txt\t cv423_12089.txt  cv673_25874.txt  cv923_11951.txt\n",
            "cv174_9735.txt\t cv424_9268.txt   cv674_11593.txt  cv924_29397.txt\n",
            "cv175_7375.txt\t cv425_8603.txt   cv675_22871.txt  cv925_9459.txt\n",
            "cv176_14196.txt  cv426_10976.txt  cv676_22202.txt  cv926_18471.txt\n",
            "cv177_10904.txt  cv427_11693.txt  cv677_18938.txt  cv927_11471.txt\n",
            "cv178_14380.txt  cv428_12202.txt  cv678_14887.txt  cv928_9478.txt\n",
            "cv179_9533.txt\t cv429_7937.txt   cv679_28221.txt  cv929_1841.txt\n",
            "cv180_17823.txt  cv430_18662.txt  cv680_10533.txt  cv930_14949.txt\n",
            "cv181_16083.txt  cv431_7538.txt   cv681_9744.txt   cv931_18783.txt\n",
            "cv182_7791.txt\t cv432_15873.txt  cv682_17947.txt  cv932_14854.txt\n",
            "cv183_19826.txt  cv433_10443.txt  cv683_13047.txt  cv933_24953.txt\n",
            "cv184_26935.txt  cv434_5641.txt   cv684_12727.txt  cv934_20426.txt\n",
            "cv185_28372.txt  cv435_24355.txt  cv685_5710.txt   cv935_24977.txt\n",
            "cv186_2396.txt\t cv436_20564.txt  cv686_15553.txt  cv936_17473.txt\n",
            "cv187_14112.txt  cv437_24070.txt  cv687_22207.txt  cv937_9816.txt\n",
            "cv188_20687.txt  cv438_8500.txt   cv688_7884.txt   cv938_10706.txt\n",
            "cv189_24248.txt  cv439_17633.txt  cv689_13701.txt  cv939_11247.txt\n",
            "cv190_27176.txt  cv440_16891.txt  cv690_5425.txt   cv940_18935.txt\n",
            "cv191_29539.txt  cv441_15276.txt  cv691_5090.txt   cv941_10718.txt\n",
            "cv192_16079.txt  cv442_15499.txt  cv692_17026.txt  cv942_18509.txt\n",
            "cv193_5393.txt\t cv443_22367.txt  cv693_19147.txt  cv943_23547.txt\n",
            "cv194_12855.txt  cv444_9975.txt   cv694_4526.txt   cv944_15042.txt\n",
            "cv195_16146.txt  cv445_26683.txt  cv695_22268.txt  cv945_13012.txt\n",
            "cv196_28898.txt  cv446_12209.txt  cv696_29619.txt  cv946_20084.txt\n",
            "cv197_29271.txt  cv447_27334.txt  cv697_12106.txt  cv947_11316.txt\n",
            "cv198_19313.txt  cv448_16409.txt  cv698_16930.txt  cv948_25870.txt\n",
            "cv199_9721.txt\t cv449_9126.txt   cv699_7773.txt   cv949_21565.txt\n",
            "cv200_29006.txt  cv450_8319.txt   cv700_23163.txt  cv950_13478.txt\n",
            "cv201_7421.txt\t cv451_11502.txt  cv701_15880.txt  cv951_11816.txt\n",
            "cv202_11382.txt  cv452_5179.txt   cv702_12371.txt  cv952_26375.txt\n",
            "cv203_19052.txt  cv453_10911.txt  cv703_17948.txt  cv953_7078.txt\n",
            "cv204_8930.txt\t cv454_21961.txt  cv704_17622.txt  cv954_19932.txt\n",
            "cv205_9676.txt\t cv455_28866.txt  cv705_11973.txt  cv955_26154.txt\n",
            "cv206_15893.txt  cv456_20370.txt  cv706_25883.txt  cv956_12547.txt\n",
            "cv207_29141.txt  cv457_19546.txt  cv707_11421.txt  cv957_9059.txt\n",
            "cv208_9475.txt\t cv458_9000.txt   cv708_28539.txt  cv958_13020.txt\n",
            "cv209_28973.txt  cv459_21834.txt  cv709_11173.txt  cv959_16218.txt\n",
            "cv210_9557.txt\t cv460_11723.txt  cv710_23745.txt  cv960_28877.txt\n",
            "cv211_9955.txt\t cv461_21124.txt  cv711_12687.txt  cv961_5578.txt\n",
            "cv212_10054.txt  cv462_20788.txt  cv712_24217.txt  cv962_9813.txt\n",
            "cv213_20300.txt  cv463_10846.txt  cv713_29002.txt  cv963_7208.txt\n",
            "cv214_13285.txt  cv464_17076.txt  cv714_19704.txt  cv964_5794.txt\n",
            "cv215_23246.txt  cv465_23401.txt  cv715_19246.txt  cv965_26688.txt\n",
            "cv216_20165.txt  cv466_20092.txt  cv716_11153.txt  cv966_28671.txt\n",
            "cv217_28707.txt  cv467_26610.txt  cv717_17472.txt  cv967_5626.txt\n",
            "cv218_25651.txt  cv468_16844.txt  cv718_12227.txt  cv968_25413.txt\n",
            "cv219_19874.txt  cv469_21998.txt  cv719_5581.txt   cv969_14760.txt\n",
            "cv220_28906.txt  cv470_17444.txt  cv720_5383.txt   cv970_19532.txt\n",
            "cv221_27081.txt  cv471_18405.txt  cv721_28993.txt  cv971_11790.txt\n",
            "cv222_18720.txt  cv472_29140.txt  cv722_7571.txt   cv972_26837.txt\n",
            "cv223_28923.txt  cv473_7869.txt   cv723_9002.txt   cv973_10171.txt\n",
            "cv224_18875.txt  cv474_10682.txt  cv724_15265.txt  cv974_24303.txt\n",
            "cv225_29083.txt  cv475_22978.txt  cv725_10266.txt  cv975_11920.txt\n",
            "cv226_26692.txt  cv476_18402.txt  cv726_4365.txt   cv976_10724.txt\n",
            "cv227_25406.txt  cv477_23530.txt  cv727_5006.txt   cv977_4776.txt\n",
            "cv228_5644.txt\t cv478_15921.txt  cv728_17931.txt  cv978_22192.txt\n",
            "cv229_15200.txt  cv479_5450.txt   cv729_10475.txt  cv979_2029.txt\n",
            "cv230_7913.txt\t cv480_21195.txt  cv730_10729.txt  cv980_11851.txt\n",
            "cv231_11028.txt  cv481_7930.txt   cv731_3968.txt   cv981_16679.txt\n",
            "cv232_16768.txt  cv482_11233.txt  cv732_13092.txt  cv982_22209.txt\n",
            "cv233_17614.txt  cv483_18103.txt  cv733_9891.txt   cv983_24219.txt\n",
            "cv234_22123.txt  cv484_26169.txt  cv734_22821.txt  cv984_14006.txt\n",
            "cv235_10704.txt  cv485_26879.txt  cv735_20218.txt  cv985_5964.txt\n",
            "cv236_12427.txt  cv486_9788.txt   cv736_24947.txt  cv986_15092.txt\n",
            "cv237_20635.txt  cv487_11058.txt  cv737_28733.txt  cv987_7394.txt\n",
            "cv238_14285.txt  cv488_21453.txt  cv738_10287.txt  cv988_20168.txt\n",
            "cv239_29828.txt  cv489_19046.txt  cv739_12179.txt  cv989_17297.txt\n",
            "cv240_15948.txt  cv490_18986.txt  cv740_13643.txt  cv990_12443.txt\n",
            "cv241_24602.txt  cv491_12992.txt  cv741_12765.txt  cv991_19973.txt\n",
            "cv242_11354.txt  cv492_19370.txt  cv742_8279.txt   cv992_12806.txt\n",
            "cv243_22164.txt  cv493_14135.txt  cv743_17023.txt  cv993_29565.txt\n",
            "cv244_22935.txt  cv494_18689.txt  cv744_10091.txt  cv994_13229.txt\n",
            "cv245_8938.txt\t cv495_16121.txt  cv745_14009.txt  cv995_23113.txt\n",
            "cv246_28668.txt  cv496_11185.txt  cv746_10471.txt  cv996_12447.txt\n",
            "cv247_14668.txt  cv497_27086.txt  cv747_18189.txt  cv997_5152.txt\n",
            "cv248_15672.txt  cv498_9288.txt   cv748_14044.txt  cv998_15691.txt\n",
            "cv249_12674.txt  cv499_11407.txt  cv749_18960.txt  cv999_14636.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNQxR009ZcWF"
      },
      "source": [
        "# **Loading and Cleaning Reviews**\n",
        "\n",
        "1.Split tokens on white space. \n",
        "\n",
        "2.Remove all punctuation from words.\n",
        "\n",
        "3.Remove all words that are not purely comprised of alphabetical characters.\n",
        "\n",
        "4.Remove all words that are known stop words. \n",
        "\n",
        "5.Remove all words that have a length â‰¤ 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHPoQRjCaVz-",
        "outputId": "d1eb11b5-f2ae-495b-f6fd-015e28f268fe"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from collections import Counter\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\n",
        "# open the file as read only \n",
        " file = open(filename, 'r')\n",
        "\n",
        "# read all text \n",
        " text = file.read()\n",
        "\n",
        "# close the file \n",
        " file.close() \n",
        " \n",
        " return text\n",
        "\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc): \n",
        "\n",
        "  # split into tokens by white space \n",
        "  tokens = doc.split() \n",
        "    \n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  # remove punctuation from each word \n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()] \n",
        "\n",
        "  # filter out stop words \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "  # filter out short tokens \n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1grRPFmYafWa"
      },
      "source": [
        "# **Define a Vocabulary:**\n",
        "\n",
        " It is important to define a vocabulary of known words when using a text model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive.\n",
        " \n",
        "1. This is difficult to know beforehand and often it is important to test different hypotheses about how to construct a useful vocabulary. We have already seen how we can remove punctuation and numbers from the vocabulary in the previous step\n",
        "\n",
        "\n",
        "2. We can repeat this for all documents and build a set of all known words. We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. \n",
        " \n",
        "3. Each document can be added to the counter (a new function called add doc_to_vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process_docs())."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYpWLPsdoosr"
      },
      "source": [
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename,vocab):\n",
        "  # load doc\n",
        "  doc=load_doc(filename)\n",
        "  #clean doc\n",
        "  tokens=clean_doc(doc)\n",
        "  # update Counts\n",
        "  vocab.update(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRHRr1CJkBC"
      },
      "source": [
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab): \n",
        "\n",
        "# walk through all files in the folder \n",
        "  for filename in os.listdir(directory):\n",
        "\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue \n",
        "\n",
        "  # create the full path of the \n",
        "    path = directory + '/' + filename\n",
        "\n",
        " # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cygieW77pPbx",
        "outputId": "01c9588c-c018-418f-dc23-dd8dc2dcb90a"
      },
      "source": [
        "\n",
        "#define vocab\n",
        "vocab=Counter()\n",
        "\n",
        "#add all docs to vocab\n",
        "process_docs('/root/.keras/datasets/txt_sentoken/neg/',vocab)\n",
        "process_docs('/root/.keras/datasets/txt_sentoken/pos/',vocab)\n",
        "\n",
        "#print size of vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print top words in vocab\n",
        "print(vocab.most_common(50))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9M_Fxn4ScyV",
        "outputId": "139c6f7e-3d74-4093-eea7-c2c53a926e3c"
      },
      "source": [
        "# keep tokens with a min occurrence\n",
        "\n",
        "min_occurence=2\n",
        "tokens=[k for k,v in vocab.items() if v>=min_occurence]\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9pqBM0-S5Py"
      },
      "source": [
        "# save list to file \n",
        "def save_list(lines, filename):\n",
        "\n",
        "   # convert lines to a single blob of text \n",
        "   data = '\\n'.join(lines) \n",
        "\n",
        "   # open file \n",
        "   file = open(filename, 'w')\n",
        "\n",
        "    # write text \n",
        "   file.write(data)\n",
        "   \n",
        "    # close file \n",
        "   file.close()\n",
        "     \n",
        "# save tokens to a vocabulary file \n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52nkmxftLNSg",
        "outputId": "9d422b93-5ad3-4c33-b4b4-2ab1bf6b7d6a"
      },
      "source": [
        "filename='/root/.keras/datasets/txt_sentoken/pos/cv000_29590.txt'\n",
        "doc=load_doc(filename)\n",
        "print(doc)\n",
        "tokens=clean_doc(doc)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
            "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
            "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
            "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
            "in other words , don't dismiss this film because of its source . \n",
            "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
            "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \n",
            "the ghetto in question is , of course , whitechapel in 1888 london's east end . \n",
            "it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \n",
            "when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \n",
            "abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \n",
            "upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \n",
            "i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \n",
            "in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \n",
            "it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \n",
            "and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \n",
            "don't worry - it'll all make sense when you see it . \n",
            "now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \n",
            "the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \n",
            "oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \n",
            "even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \n",
            "ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \n",
            "i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \n",
            "the film , however , is all good . \n",
            "2 : 00 - r for strong violence/gore , sexuality , language and drug content \n",
            "\n",
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcBgxaldMJj_"
      },
      "source": [
        "def load_doc(filename):\n",
        "  file=open(filename,'r')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNv051kYaw2N"
      },
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TviAiCSerjh"
      },
      "source": [
        "# **Train CNN With Embedding Layer**\n",
        "\n",
        "In this step we will learn a word embedding while training a convolutional neural network on the classification problem.\n",
        "\n",
        "\n",
        " A word embedding is a way of representing text where each word in the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space (close in the vector space). \n",
        " \n",
        "This is a more expressive representation for text than more classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches. \n",
        "\n",
        "\n",
        "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer. \n",
        "\n",
        "The f irst step is to load the vocabulary. We will use it to filter out words from movie reviews that we are not interested in. \n",
        "\n",
        "In the previous section,  local file called vocab.txt with one word per line. We can load that file and build a vocabulary as a set for checking the validity of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGkLkw-LUj_7"
      },
      "source": [
        "\n",
        "def load_doc(filename):\n",
        "  file=open(filename,'r')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "def clean_doc(doc,vocab):\n",
        "  tokens=doc.split()\n",
        "  re_punc=re.compile('[%s]'%re.escape(string.punctuation))\n",
        "  tokens=[re_punc.sub('',w) for w in tokens]\n",
        "  tokens=[word.lower() for word in tokens]\n",
        "\n",
        "  tokens=[word for word in tokens if word.isalpha()]\n",
        "  #tokens=[word for word in tokens if not word in stop_words]\n",
        "  tokens=[word for word in tokens if word in vocab]\n",
        "  tokens=' '.join(tokens)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BsSTiKPWMHh"
      },
      "source": [
        "def preprocess_doc(directory,vocab,is_train):\n",
        "  documents=list()\n",
        "\n",
        "  for filename in os.listdir(directory):\n",
        "\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    \n",
        "    path=directory +'/'+filename\n",
        "\n",
        "    doc=load_doc(path)\n",
        "    tokens=clean_doc(doc,vocab)\n",
        "    documents.append(tokens)\n",
        "  return documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIySrycPfbQw"
      },
      "source": [
        "filename='vocab.txt'\n",
        "vocabt=load_doc(filename)\n",
        "vocab=set(vocabt.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX3yiRP-Yaie"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuvAKA7zffbP"
      },
      "source": [
        "We can call the process clean docs function for both the neg and pos directories and combine the reviews into a single train or test dataset. \n",
        "\n",
        "We also can define the class labels for the dataset. The load dataset() function below will load all reviews and prepare class labels for the training or test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJa68pMXiau"
      },
      "source": [
        "def load_clean_doc(vocab,is_train):\n",
        "  # load documents\n",
        "  neg=preprocess_doc('/root/.keras/datasets/txt_sentoken/neg/',vocab,is_train)\n",
        "  pos=preprocess_doc('/root/.keras/datasets/txt_sentoken/pos/',vocab,is_train)\n",
        "  docs =neg+pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels=np.array([0 for _ in range(len(neg))]+[1 for _ in range(len(pos))])\n",
        "  return docs,labels\n",
        "\n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFTrGxxhjzwU",
        "outputId": "f57c158c-8674-4b0c-ff09-530be5402f70"
      },
      "source": [
        "model=define_model(vocab_size,max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFg867EXgB1v"
      },
      "source": [
        "train_docs,ytrain=load_clean_doc(vocab,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_xvr0YOgJIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa4d2c7-069b-49db-ac3e-6443bbb711b5"
      },
      "source": [
        "train_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYlJzCZfxJU"
      },
      "source": [
        "The next step is to encode each document as a sequence of integers. \n",
        "\n",
        "The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector representation within the embedding. These vectors are random at the beginning of training, but during training become meaningful to the network. \n",
        "\n",
        "We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API. First, we must construct an instance of the class then train it on all documents in the training dataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops a consistent mapping from words in the vocabulary to unique integers. \n",
        "\n",
        "We could just as easily develop this mapping ourselves using our vocabulary file. The create below will prepare a Tokenizer from the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P4zVHwUgiqv"
      },
      "source": [
        "def create_tokenizer(lines):\n",
        "  tokenizer=Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpHEHPPug-SA"
      },
      "source": [
        "tokenizer=create_tokenizer(train_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNhZkTxhGP1",
        "outputId": "69d44c1c-e0ed-4ae4-ddc3-2c7e2be91e9f"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7facebf92610>"
            ]
          },
          "metadata": {},
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zFEVaYbgKhU"
      },
      "source": [
        "Now that the mapping of words to integers has been prepared, we can use it to encode the reviews in the training dataset. We can do that by calling the texts_to_sequences() function on the Tokenizer.\n",
        "\n",
        " We also need to ensure that all documents have the same length. This is a requirement of Keras for efficient computation. We could truncate reviews to the smallest size or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid.\n",
        " \n",
        "  In this case, we will pad all reviews to the length of the longest review in the training dataset. First, we can f ind the longest review using the max() function on the training dataset and take its length. We can then call the Keras function pad length by adding 0 values on the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibobDsUBhjtu"
      },
      "source": [
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer,max_length,docs):\n",
        "  # integer encode\n",
        "  encoded=tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
        "  return padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0AwvGIUhOUK",
        "outputId": "786eb942-e976-487a-a9c9-7982ab1f48fe"
      },
      "source": [
        "\n",
        "# calculate the maximum sequence length\n",
        "max_length=max([len(s.split()) for s in train_docs])\n",
        "print('Max length is ',max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length is  1317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqa0NJyMhHfY",
        "outputId": "92c8f285-e47d-4638-9000-4867464a6ccf"
      },
      "source": [
        "# define vocabulary size\n",
        "#Calculate the size of the vocabulary for the Embedding layer.\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25768"
            ]
          },
          "metadata": {},
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saz21cbBg78I"
      },
      "source": [
        "## **Defining model architechture**\n",
        "\n",
        "We will use a 100-dimensional vector space, but you could try other values, such as 50 or 150. Finally, the maximum document length was calculated above in the max length variable used during padding.\n",
        "\n",
        " The complete model definition is listed below including the Embedding layer. We use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems. \n",
        " \n",
        "A conservative CNN configuration is used with 32 filters (parallel fields for processing words) and a kernel size of 8 with a rectified linear (relu) activation function. This is followed by a pooling layer that reduces the output of the convolutional layer by half. \n",
        "\n",
        "Next, the 2D output from the CNN part of the model is flattened to one long 2D vector to represent the features extracted by the CNN. \n",
        "\n",
        "The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. \n",
        "\n",
        "The output layer uses a sigmoid activation function to output a value between 0 and 1 for the negative and positive sentiment in the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvSdMyYZYxCv"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Flatten,Dense,MaxPooling1D,Embedding,Conv1D\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pEeQXepeEvk"
      },
      "source": [
        "def define_model(vocab_size,max_length):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(vocab_size,100,input_length=max_length))\n",
        "  model.add(Conv1D(32,8 ,activation='relu')) \n",
        "  # n=32 number of filter for conv1d\n",
        "  #m=1317 number of words in sentence\n",
        "  #k=8 filter size. output shape number_filters(kernel_size*embed_dim)\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10,activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt2Zhez2iAyH"
      },
      "source": [
        "Xtrain=encode_docs(tokenizer,max_length,train_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0SfXicIiN66",
        "outputId": "8500f13e-7dc6-40a5-eb0e-92fe583cc666"
      },
      "source": [
        "Xtrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1492,     1,  1297, ...,     0,     0,     0],\n",
              "       [  214,  1284,     1, ...,     0,     0,     0],\n",
              "       [    3,   263,   681, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  185, 10975,   716, ...,     0,     0,     0],\n",
              "       [ 3731,  9258,  1943, ...,     0,     0,     0],\n",
              "       [ 2564,  5715,   349, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 480
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu-hjZ-Kh2DF"
      },
      "source": [
        "we fit the network on the training data. We use a binary cross-entropy loss function because the problem we are learning is a binary classification problem. The efficient Adam implementation of stochastic gradient descent is used and we keep track of accuracy in addition to loss during training. The model is trained for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svTbYR5uiPmU",
        "outputId": "406beff5-d53e-4be5-cbf8-3c6b066fb1a1"
      },
      "source": [
        "# fit network\n",
        "model.fit(Xtrain,ytrain,epochs=10,verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "57/57 - 1s - loss: 0.6934 - accuracy: 0.5178\n",
            "Epoch 2/10\n",
            "57/57 - 0s - loss: 0.5922 - accuracy: 0.6750\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.1572 - accuracy: 0.9483\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.0124 - accuracy: 0.9983\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 9.5671e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "57/57 - 0s - loss: 7.4839e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 6.0934e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 5.1145e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7facecc1a2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 481
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bCUSea5iUxu"
      },
      "source": [
        "model.save('model.h5') # save the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSskpKhMlgO3"
      },
      "source": [
        "train_doc,ytrain=load_clean_doc(vocab,True)\n",
        "test_doc,y_test=load_clean_doc(vocab,False)\n",
        "Xtest=encode_docs(tokenizer,max_length,test_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00vauCvzl9pS",
        "outputId": "2c2d19e2-9cad-4043-e764-035485b489cd"
      },
      "source": [
        "# load the model\n",
        "#model=load_model('model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "_,acc=model.evaluate(Xtrain,ytrain,verbose=0)\n",
        "print('train accuracy',(acc*100))\n",
        "\n",
        "\n",
        "# evaluate model on test dataset\n",
        "\n",
        "_,acc=model.evaluate(Xtest,y_test,verbose=0)\n",
        "print('Test accuracy',(acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 100.0\n",
            "Test accuracy 88.49999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUjJMwFu00mS",
        "outputId": "7d807367-da1f-4764-d46a-239dcedf2c2a"
      },
      "source": [
        "model.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.3639782e-02],\n",
              "       [9.4005809e-04],\n",
              "       [6.4791783e-07],\n",
              "       [1.5333651e-05],\n",
              "       [3.9064748e-06],\n",
              "       [4.3336245e-06],\n",
              "       [1.3360527e-01],\n",
              "       [2.3614551e-04],\n",
              "       [5.0739106e-07],\n",
              "       [7.8642982e-01],\n",
              "       [2.1959063e-06],\n",
              "       [1.6228665e-05],\n",
              "       [2.1453020e-03],\n",
              "       [1.5732989e-02],\n",
              "       [1.4940880e-03],\n",
              "       [1.2398261e-03],\n",
              "       [1.9317281e-02],\n",
              "       [2.6634575e-03],\n",
              "       [9.9558300e-01],\n",
              "       [3.5103521e-01],\n",
              "       [1.2156356e-02],\n",
              "       [6.4676057e-04],\n",
              "       [6.1596598e-04],\n",
              "       [7.3108450e-04],\n",
              "       [2.5078602e-05],\n",
              "       [6.7723191e-01],\n",
              "       [2.0167528e-05],\n",
              "       [6.0389412e-04],\n",
              "       [2.1343967e-06],\n",
              "       [1.5003771e-01],\n",
              "       [2.0404351e-03],\n",
              "       [1.7327410e-06],\n",
              "       [1.7500315e-02],\n",
              "       [2.9757783e-01],\n",
              "       [8.5410334e-02],\n",
              "       [1.0875522e-03],\n",
              "       [1.5935737e-03],\n",
              "       [6.9199261e-05],\n",
              "       [7.5063581e-05],\n",
              "       [4.8229420e-05],\n",
              "       [4.1458725e-07],\n",
              "       [8.8929016e-07],\n",
              "       [1.9517065e-04],\n",
              "       [1.1478378e-08],\n",
              "       [3.3690901e-03],\n",
              "       [3.8982683e-01],\n",
              "       [5.5149249e-05],\n",
              "       [2.1856646e-05],\n",
              "       [2.1859189e-06],\n",
              "       [1.0314498e-05],\n",
              "       [2.3795615e-01],\n",
              "       [2.5147080e-02],\n",
              "       [1.1108911e-02],\n",
              "       [1.5816151e-04],\n",
              "       [6.7387694e-01],\n",
              "       [1.4514633e-03],\n",
              "       [7.3781013e-02],\n",
              "       [8.9669753e-05],\n",
              "       [2.6221690e-01],\n",
              "       [1.9548016e-02],\n",
              "       [1.3865152e-01],\n",
              "       [7.5348324e-01],\n",
              "       [1.6466033e-04],\n",
              "       [5.1537598e-04],\n",
              "       [5.4779762e-01],\n",
              "       [8.1399111e-07],\n",
              "       [2.0318029e-03],\n",
              "       [1.0264805e-03],\n",
              "       [1.3400750e-01],\n",
              "       [2.1976075e-06],\n",
              "       [3.9543301e-02],\n",
              "       [2.4380912e-04],\n",
              "       [1.5630956e-01],\n",
              "       [7.9902173e-05],\n",
              "       [2.7584833e-01],\n",
              "       [1.5741165e-04],\n",
              "       [2.6504865e-01],\n",
              "       [1.3155267e-06],\n",
              "       [2.1425616e-04],\n",
              "       [9.9970263e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.1703225e-02],\n",
              "       [2.2196770e-03],\n",
              "       [1.5322966e-06],\n",
              "       [2.4964729e-05],\n",
              "       [1.1741678e-05],\n",
              "       [1.2117117e-02],\n",
              "       [6.4635915e-03],\n",
              "       [8.9066831e-04],\n",
              "       [8.3255246e-02],\n",
              "       [5.7544139e-07],\n",
              "       [9.1658785e-07],\n",
              "       [2.5531496e-03],\n",
              "       [2.2284154e-04],\n",
              "       [4.9770409e-03],\n",
              "       [1.6768839e-06],\n",
              "       [6.0258345e-03],\n",
              "       [5.4602569e-01],\n",
              "       [2.6067087e-04],\n",
              "       [9.9990129e-01],\n",
              "       [9.9997842e-01],\n",
              "       [9.9999988e-01],\n",
              "       [3.4274585e-03],\n",
              "       [9.9981076e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.4195177e-01],\n",
              "       [9.9995363e-01],\n",
              "       [3.5157505e-01],\n",
              "       [9.5235264e-01],\n",
              "       [9.1599971e-01],\n",
              "       [9.9971586e-01],\n",
              "       [9.4533587e-01],\n",
              "       [9.9912804e-01],\n",
              "       [9.9900681e-01],\n",
              "       [2.1700964e-03],\n",
              "       [9.8420322e-01],\n",
              "       [9.9962485e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.3480945e-01],\n",
              "       [9.9970645e-01],\n",
              "       [9.9990630e-01],\n",
              "       [9.9863476e-01],\n",
              "       [9.9992418e-01],\n",
              "       [9.9979025e-01],\n",
              "       [9.9711323e-01],\n",
              "       [9.9999666e-01],\n",
              "       [9.9506783e-01],\n",
              "       [9.9992716e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.0000000e+00],\n",
              "       [9.9990523e-01],\n",
              "       [9.9967098e-01],\n",
              "       [6.5946323e-01],\n",
              "       [6.3055031e-02],\n",
              "       [2.0487143e-01],\n",
              "       [9.9997747e-01],\n",
              "       [9.9999022e-01],\n",
              "       [9.7753704e-01],\n",
              "       [5.5944282e-01],\n",
              "       [9.8421878e-01],\n",
              "       [9.9999964e-01],\n",
              "       [8.0353200e-01],\n",
              "       [9.9990344e-01],\n",
              "       [8.5177964e-01],\n",
              "       [9.9999213e-01],\n",
              "       [2.7728187e-02],\n",
              "       [9.9891508e-01],\n",
              "       [9.3033177e-01],\n",
              "       [9.1145295e-01],\n",
              "       [8.0858938e-02],\n",
              "       [7.1562260e-01],\n",
              "       [8.7780678e-01],\n",
              "       [9.9995625e-01],\n",
              "       [9.9999249e-01],\n",
              "       [7.6416683e-01],\n",
              "       [9.9999547e-01],\n",
              "       [9.9975961e-01],\n",
              "       [7.9030275e-01],\n",
              "       [2.4347844e-02],\n",
              "       [9.9913067e-01],\n",
              "       [9.9878258e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.5332867e-01],\n",
              "       [9.9992371e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.9065924e-01],\n",
              "       [9.8946840e-01],\n",
              "       [9.9924481e-01],\n",
              "       [9.9976557e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.7541535e-01],\n",
              "       [5.0841659e-01],\n",
              "       [9.9809903e-01],\n",
              "       [7.8357226e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.9893230e-01],\n",
              "       [9.9999940e-01],\n",
              "       [9.9998152e-01],\n",
              "       [9.9997902e-01],\n",
              "       [9.9174386e-01],\n",
              "       [9.9978536e-01],\n",
              "       [9.9999452e-01],\n",
              "       [2.9116957e-02],\n",
              "       [1.5569243e-01],\n",
              "       [9.9999988e-01],\n",
              "       [9.9999976e-01],\n",
              "       [1.0000000e+00],\n",
              "       [1.0000000e+00],\n",
              "       [3.8033977e-01],\n",
              "       [8.7899584e-01],\n",
              "       [1.7506637e-02],\n",
              "       [9.9989855e-01],\n",
              "       [9.9860460e-01],\n",
              "       [9.9998510e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.9742329e-01],\n",
              "       [7.8732693e-01],\n",
              "       [9.9912661e-01],\n",
              "       [9.7735083e-01],\n",
              "       [5.6197786e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSV3UvroiuYZ"
      },
      "source": [
        "## **Function to predict the sentiment for an ad hoc movie review**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJE9M1r4maPX"
      },
      "source": [
        "# classify a review as negative or positive\n",
        "\n",
        "def predict_review(review,vocab,tokenizer,max_length,model):\n",
        "  # clean review\n",
        "  line=clean_doc(review,vocab)\n",
        "\n",
        "  # encode and pad review\n",
        "  padded=encode_docs(tokenizer,max_length,[line])\n",
        "\n",
        "  # predict sentiment\n",
        "  yhat=model.predict(padded,verbose=0)\n",
        "\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos=yhat[0,0]\n",
        "  if round(percent_pos)==0:\n",
        "    return (1-percent_pos),'NEG'\n",
        "  \n",
        "  return percent_pos, 'POS'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSzklyeqnhP_"
      },
      "source": [
        "text='This is bad movie. Donot watch it'\n",
        "percent,sentiment=predict_review(text,vocab,tokenizer,max_length,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcpv0Evzn7NN",
        "outputId": "5c532de9-7f44-410b-f1e0-7dad38ffc980"
      },
      "source": [
        "percent,sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5455211997032166, 'NEG')"
            ]
          },
          "metadata": {},
          "execution_count": 488
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hLFET0goByG"
      },
      "source": [
        "text='Everyone will enjoy this film. I love it, recommended!'\n",
        "percent1,sentiment1=predict_review(text,vocab,tokenizer,max_length,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se0AZgGeu9y7",
        "outputId": "d6217a50-1d3c-4771-efb0-96b07d9e7bbe"
      },
      "source": [
        "percent1,sentiment1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5075193, 'POS')"
            ]
          },
          "metadata": {},
          "execution_count": 490
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX41IWeL2u5L"
      },
      "source": [
        "# **Conclusion:**\n",
        "\n",
        "\n",
        "Running the example first prints the skill of the model on the training and test dataset. We can see that the model achieves 100% accuracy on the training dataset and 87.5% on the test dataset, an impressive score. \n",
        "\n",
        " Next, we can see that the model makes the correct prediction on two contrived movie reviews. We can see that the percentage or confidence of the prediction is close to 50% for both, this may be because the two contrived reviews are very short and the model is expecting sequences of 1,000 or more words."
      ]
    }
  ]
}